
=============================================================================
GATOLINK PRIORITY SOFTWARE DEPLOYMENT GUIDES
=============================================================================

Based on awesome-selfhosted analysis and your Cortejo infrastructure needs.

-----------------------------------------------------------------------------
1. vLLM - HIGH-PERFORMANCE LLM INFERENCE ENGINE
-----------------------------------------------------------------------------

PURPOSE: Serve Llama 3.1 70B and other large models on RTX 6000 Ada (144GB VRAM)

DOCKER DEPLOYMENT:

# Pull official vLLM image
docker pull vllm/vllm-openai:latest

# Run vLLM with Llama 3.1 70B
docker run --runtime nvidia --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=<your-token>" \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Meta-Llama-3.1-70B-Instruct \
  --tensor-parallel-size 1 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.90

DOCKER COMPOSE VERSION:

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: gato-vllm-llama70b
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '16gb'
    command: >
      --model meta-llama/Meta-Llama-3.1-70B-Instruct
      --tensor-parallel-size 1
      --max-model-len 8192
      --gpu-memory-utilization 0.90
      --served-model-name llama-70b

TESTING:
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-70b",
    "prompt": "What is AI inference?",
    "max_tokens": 100
  }'

INTEGRATION NOTES:
- OpenAI-compatible API at localhost:8000/v1
- Supports streaming, chat completions, embeddings
- Monitor GPU usage with nvidia-smi
- Log output with: docker logs gato-vllm-llama70b


-----------------------------------------------------------------------------
2. LAGO - USAGE-BASED BILLING & METERING
-----------------------------------------------------------------------------

PURPOSE: Track API token usage, GPU hours, and generate invoices

DOCKER COMPOSE DEPLOYMENT:

version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: lago-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: lago
      POSTGRES_USER: lago
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - lago_postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    container_name: lago-redis
    restart: unless-stopped
    volumes:
      - lago_redis_data:/data
    ports:
      - "6379:6379"

  lago-api:
    image: getlago/api:latest
    container_name: lago-api
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
    environment:
      DATABASE_URL: postgresql://lago:${POSTGRES_PASSWORD}@postgres:5432/lago
      REDIS_URL: redis://redis:6379
      LAGO_API_KEY: ${LAGO_API_KEY}
      SECRET_KEY_BASE: ${SECRET_KEY_BASE}
      LAGO_FRONT_URL: http://localhost:3000
    ports:
      - "3000:3000"
    volumes:
      - lago_storage:/app/storage

  lago-front:
    image: getlago/front:latest
    container_name: lago-front
    restart: unless-stopped
    depends_on:
      - lago-api
    environment:
      LAGO_API_URL: http://lago-api:3000
    ports:
      - "8080:80"

volumes:
  lago_postgres_data:
  lago_redis_data:
  lago_storage:

ENV FILE (.env):
POSTGRES_PASSWORD=your_secure_postgres_password
LAGO_API_KEY=your_lago_api_key_here
SECRET_KEY_BASE=$(openssl rand -hex 64)

STARTUP:
docker-compose up -d

ACCESS:
- Dashboard: http://localhost:8080
- API: http://localhost:3000/api/v1

INTEGRATION WITH GATOLINK:
# Send usage events to Lago
curl -X POST http://localhost:3000/api/v1/events \
  -H "Authorization: Bearer ${LAGO_API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "event": {
      "transaction_id": "unique-event-id",
      "external_customer_id": "customer-123",
      "code": "gpu_inference_tokens",
      "timestamp": 1699000000,
      "properties": {
        "tokens": 1024,
        "model": "llama-70b"
      }
    }
  }'


-----------------------------------------------------------------------------
3. QDRANT - VECTOR DATABASE FOR RAG & MEMORY
-----------------------------------------------------------------------------

PURPOSE: Store embeddings for retrieval-augmented generation (RAG)

DOCKER DEPLOYMENT:

docker run -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage:z \
  -e QDRANT__SERVICE__API_KEY=your_secret_api_key \
  qdrant/qdrant:latest

DOCKER COMPOSE VERSION:

services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: gato-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - ./qdrant_storage:/qdrant/storage
      - ./qdrant_config:/qdrant/config
    environment:
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
      - QDRANT__LOG_LEVEL=INFO
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

GENERATE API KEY:
openssl rand -hex 32

TESTING:
curl http://localhost:6333/healthz

CREATE A COLLECTION:
curl -X PUT http://localhost:6333/collections/gato-memory \
  -H "Content-Type: application/json" \
  -H "api-key: your_secret_api_key" \
  -d '{
    "vectors": {
      "size": 1536,
      "distance": "Cosine"
    }
  }'

INSERT VECTORS:
curl -X PUT http://localhost:6333/collections/gato-memory/points \
  -H "Content-Type: application/json" \
  -H "api-key: your_secret_api_key" \
  -d '{
    "points": [
      {
        "id": 1,
        "vector": [0.1, 0.2, 0.3, ...],
        "payload": {"text": "Sample document"}
      }
    ]
  }'


=============================================================================
BONUS: COMPLETE INFRASTRUCTURE STACK
=============================================================================

FULL DOCKER COMPOSE FOR GATOLINK:

version: '3.8'

services:
  # Core Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: gato-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    container_name: gato-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

  # Reverse Proxy
  traefik:
    image: traefik:v2.10
    container_name: gato-traefik
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik.yml:/etc/traefik/traefik.yml
      - ./acme.json:/acme.json

  # Inference
  vllm:
    image: vllm/vllm-openai:latest
    container_name: gato-vllm
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model meta-llama/Meta-Llama-3.1-70B-Instruct
      --max-model-len 8192

  # Vector DB
  qdrant:
    image: qdrant/qdrant:latest
    container_name: gato-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}

  # Database
  postgres:
    image: postgres:15-alpine
    container_name: gato-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: gatolink
      POSTGRES_USER: gato
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    container_name: gato-redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

volumes:
  prometheus_data:
  grafana_data:
  qdrant_data:
  postgres_data:
  redis_data:

DEPLOYMENT ORDER:
1. Start monitoring: docker-compose up -d prometheus grafana
2. Start data layer: docker-compose up -d postgres redis qdrant
3. Start inference: docker-compose up -d vllm
4. Start proxy: docker-compose up -d traefik

